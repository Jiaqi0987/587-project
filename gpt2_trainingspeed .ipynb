{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Define the model"
      ],
      "metadata": {
        "id": "4VCC6snkkbPE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-wyJQhLEte16"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import inspect\n",
        "import requests\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "class MultiHeadCausalAttention(nn.Module):\n",
        "    \"\"\"Implements multi-head attention with causal masking.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(MultiHeadCausalAttention, self).__init__()\n",
        "        if config.embedding_dim % config.num_heads != 0:\n",
        "            raise ValueError(\"Embedding size must be divisible by the number of heads.\")\n",
        "        # Linear transformations for query, key, and value\n",
        "        self.qkv_projection = nn.Linear(config.embedding_dim, 3 * config.embedding_dim)\n",
        "        # Linear transformation for output\n",
        "        self.final_projection = nn.Linear(config.embedding_dim, config.embedding_dim)\n",
        "        self.final_projection.SCALE_FACTOR = 1  # Custom scaling for initialization\n",
        "\n",
        "        # Number of attention heads and dimensions per head\n",
        "        self.num_heads = config.num_heads\n",
        "        self.dim_per_head = config.embedding_dim // config.num_heads\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch_size, seq_length, embed_dim = inputs.size()\n",
        "        # Compute query, key, and value tensors\n",
        "        qkv = self.qkv_projection(inputs)\n",
        "        q, k, v = torch.chunk(qkv, 3, dim=2)\n",
        "\n",
        "        # Reshape tensors for multi-head attention\n",
        "        q = q.view(batch_size, seq_length, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
        "        k = k.view(batch_size, seq_length, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_length, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
        "\n",
        "        # Perform scaled dot-product attention with causal masking\n",
        "        #att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        #att = att.masked_fill(torch.tril(torch.ones_like(att)) == 0, float('-inf'))\n",
        "        #attn_output = F.softmax(att, dim=-1) @ v\n",
        "        # use fast attention instead, which is faster\n",
        "        attn_output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        # Reshape and project back to original embedding size\n",
        "        attn_output = attn_output.transpose(1, 2).reshape(batch_size, seq_length, embed_dim)\n",
        "        return self.final_projection(attn_output)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"Applies a feed-forward network to the input tensor.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(config.embedding_dim, 4 * config.embedding_dim)\n",
        "        self.activation_fn = nn.GELU(approximate='tanh')\n",
        "        self.linear2 = nn.Linear(4 * config.embedding_dim, config.embedding_dim)\n",
        "        self.linear2.SCALE_FACTOR = 1  # Custom scaling for initialization\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation_fn(x)\n",
        "        return self.linear2(x)\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    \"\"\"Defines a single layer of the transformer.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        # Normalization layers\n",
        "        self.pre_attn_norm = nn.LayerNorm(config.embedding_dim)\n",
        "        self.pre_ffn_norm = nn.LayerNorm(config.embedding_dim)\n",
        "\n",
        "        # Attention and feed-forward components\n",
        "        self.causal_attention = MultiHeadCausalAttention(config)\n",
        "        self.feedforward = PositionwiseFeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Residual connection with attention\n",
        "        x = x + self.causal_attention(self.pre_attn_norm(x))\n",
        "        # Residual connection with feed-forward\n",
        "        x = x + self.feedforward(self.pre_ffn_norm(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class TransformerConfig:\n",
        "    \"\"\"Configuration for the Transformer model.\"\"\"\n",
        "    seq_len: int = 1024  # Maximum sequence length\n",
        "    vocab_size: int = 50257  # Vocabulary size\n",
        "    num_layers: int = 12  # Number of transformer layers\n",
        "    num_heads: int = 12  # Number of attention heads\n",
        "    embedding_dim: int = 768  # Dimensionality of embeddings\n",
        "\n",
        "    def get_block_size(self):\n",
        "        \"\"\"Returns the block size for the transformer.\"\"\"\n",
        "        return self.seq_len\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"Full GPT model with transformer architecture.\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(GPT, self).__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Embedding layers\n",
        "        self.token_embed = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
        "        self.positional_embed = nn.Embedding(config.seq_len, config.embedding_dim)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.layers = nn.ModuleList([TransformerLayer(config) for _ in range(config.num_layers)])\n",
        "        self.final_norm = nn.LayerNorm(config.embedding_dim)\n",
        "\n",
        "        # Output layer\n",
        "        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.token_embed.weight  # Weight sharing with token embedding\n",
        "\n",
        "        # Initialize parameters\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Custom initialization for model parameters.\"\"\"\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                std = 0.02\n",
        "                if hasattr(module, 'SCALE_FACTOR'):\n",
        "                    std *= (2 * self.config.num_layers) ** -0.5\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids, targets=None):\n",
        "        \"\"\"Forward pass of the GPT model.\"\"\"\n",
        "        B, T = input_ids.size()\n",
        "        if T > self.config.get_block_size():\n",
        "            raise ValueError(f\"Input sequence length {T} exceeds block size {self.config.get_block_size()}.\")\n",
        "\n",
        "        # Embedding lookup\n",
        "        positions = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
        "        x = self.token_embed(input_ids) + self.positional_embed(positions)\n",
        "\n",
        "        # Pass through transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Apply final normalization\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Compute loss if targets are provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def load_pretrained(cls, model_type):\n",
        "        \"\"\"Load pretrained GPT-2 weights from HuggingFace.\"\"\"\n",
        "        from transformers import GPT2LMHeadModel\n",
        "\n",
        "        # Supported GPT-2 models\n",
        "        model_mapping = {\n",
        "            \"gpt2\": {\"num_layers\": 12, \"num_heads\": 12, \"embedding_dim\": 768},\n",
        "            \"gpt2-medium\": {\"num_layers\": 24, \"num_heads\": 16, \"embedding_dim\": 1024},\n",
        "            \"gpt2-large\": {\"num_layers\": 36, \"num_heads\": 20, \"embedding_dim\": 1280},\n",
        "            \"gpt2-xl\": {\"num_layers\": 48, \"num_heads\": 25, \"embedding_dim\": 1600},\n",
        "        }\n",
        "\n",
        "        # Check if model type is valid\n",
        "        if model_type not in model_mapping:\n",
        "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
        "\n",
        "        # Set configuration\n",
        "        config_params = model_mapping[model_type]\n",
        "        config_params.update({\"vocab_size\": 50257, \"seq_len\": 1024})\n",
        "        config = TransformerConfig(**config_params)\n",
        "\n",
        "        # Initialize our model\n",
        "        model = cls(config)\n",
        "\n",
        "        # Load HuggingFace GPT-2 model\n",
        "        hf_model = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        hf_state_dict = hf_model.state_dict()\n",
        "\n",
        "        # Transfer weights from HuggingFace model\n",
        "        own_state_dict = model.state_dict()\n",
        "        for name, param in hf_state_dict.items():\n",
        "            if name in own_state_dict and param.size() == own_state_dict[name].size():\n",
        "                own_state_dict[name].copy_(param)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, lr, device_type):\n",
        "        \"\"\"Configure optimizer with weight decay for model parameters.\"\"\"\n",
        "        # Separate parameters into groups for weight decay\n",
        "        decay_params = []\n",
        "        no_decay_params = []\n",
        "        for name, param in self.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                if param.ndimension() >= 2:\n",
        "                    decay_params.append(param)\n",
        "                else:\n",
        "                    no_decay_params.append(param)\n",
        "\n",
        "        optimizer_groups = [\n",
        "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "            {\"params\": no_decay_params, \"weight_decay\": 0.0},\n",
        "        ]\n",
        "\n",
        "        # Use fused AdamW if supported\n",
        "        try:\n",
        "            use_fused = \"fused\" in inspect.signature(torch.optim.AdamW).parameters and device_type == \"cuda\"\n",
        "        except Exception:\n",
        "            use_fused = False\n",
        "\n",
        "        print(f\"Using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            optimizer_groups, lr=lr, betas=(0.9, 0.95), eps=1e-8, fused=use_fused\n",
        "        )\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Show some sample sequences using pretrained gpt2 weights"
      ],
      "metadata": {
        "id": "eN5QrngMkltn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IeegXy2X9TF",
        "outputId": "e07a87a0-8a60-432b-91ca-665f77d47e1f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from codecs import encode\n",
        "# check if gpu is available\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# generate some sample sequence using pretrained gpt2 weights\n",
        "num_return_sequence =5\n",
        "max_length = 15\n",
        "# generate some sample sequence using pretrained gpt2 weights\n",
        "model = GPT.load_pretrained(\"gpt2\")\n",
        "# generate using random non-trained model\n",
        "#model = GPT(TransformerConfig())\n",
        "model.eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# tokenize the text\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"I am a language model,\")\n",
        "x = torch.tensor(tokens,dtype = torch.long).unsqueeze(0).repeat(num_return_sequence,1).to(device)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "  with torch.no_grad():\n",
        "    logits,_  = model(x)\n",
        "    logits = logits[:, -1, :]\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    # select next token from top k probabilities\n",
        "    topkprobs, topkindices = torch.topk(probs, 50, dim=-1)\n",
        "    next_token_id = torch.multinomial(topkprobs, num_samples=1)\n",
        "    next_token = topkindices.gather(dim=-1, index=next_token_id)\n",
        "    # add next token to sequence\n",
        "    x = torch.cat((x, next_token), dim=1)\n",
        "# print generated sequence\n",
        "for i in range(num_return_sequence):\n",
        "  print(enc.decode(x[i].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Enb31WtdtqsZ",
        "outputId": "34485166-151c-45e5-efe0-ff426b6c2773"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a language model,,,,,:::::\n",
            "I am a language model,,,,,,,,,,\n",
            "I am a language model,,,,,,,,,,\n",
            "I am a language model,,,,,,,:::\n",
            "I am a language model,,,,,,,,,,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Train the model\n",
        "\n",
        "## 2.1 Prepare the data (using shakespear dataset)"
      ],
      "metadata": {
        "id": "f_msRR--kui4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the data\n",
        "# download the tiny shakespeare dataset and separate it for train and validation\n",
        "# this part of code is copied from github https://github.com/karpathy/nanoGPT\n",
        "input_file_path = os.path.join(os.path.dirname(\"/content/sample_data\"), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    with open(input_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "    data = f.read()\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "# encode with tiktoken gpt2 bpe\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(\"/content/sample_data\"), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(\"/content/sample_data\"), 'val.bin'))\n",
        "\n",
        "# train.bin has 301,966 tokens\n",
        "# val.bin has 36,059 tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShW9tI7LeiRv",
        "outputId": "0c265c00-46f6-408b-aedc-334bcd37fbd2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Training on Shakespear dataset"
      ],
      "metadata": {
        "id": "es5HCr4AlvqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first define the DataLoader class to load the data in batches\n",
        "class DataLoader:\n",
        "  #B is for batch size, T is for block size\n",
        "  def __init__(self, data, B, T):\n",
        "    self.tokens = torch.tensor(data,dtype = torch.int64)\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "    self.num_batches = len(data) // (B * T)\n",
        "    print(f\"Number of batches: {self.num_batches}\")\n",
        "    print(f\"Number of tokens: {len(data)}\")\n",
        "    # set position at the begin\n",
        "    self.current_position = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    B = self.B\n",
        "    T = self.T\n",
        "    buf = self.tokens [self.current_position:self.current_position + B * T+1]\n",
        "    x = buf[:-1].view(B, T)\n",
        "    y = buf[1:].view(B, T)\n",
        "    self.current_position += B * T\n",
        "\n",
        "    # if the remaining data does not enough for one batch, reset\n",
        "    if self.current_position + B * T+1 > len(self.tokens):\n",
        "      self.current_position = 0\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "-gKo25KRPgJ5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "hxVrYsXJY37K",
        "outputId": "cd214d6c-4ce9-4ecf-a7d0-d6c9ca382c6b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "Successfully installed triton-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "triton"
                ]
              },
              "id": "917eb22804e24c9f88e82ba64d36a39c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this part of code is to playwith the method for speeding up and get the average  runtime\n",
        "import triton\n",
        "# check if gpu is available\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "print(f\"Using device: {device}\")\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "B = 16\n",
        "T = 1024\n",
        "train_loader = DataLoader(train_ids, B,T)\n",
        "\n",
        "# to speed up, downgrade the precision for matrix multiplication\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# change the vocab size to nice number 50304, which can be divided by 2\n",
        "# the original vocab size is 50257\n",
        "model = GPT(TransformerConfig(vocab_size= 50304))\n",
        "model = model.to(device)\n",
        "\n",
        "#using torch.compile also helps for improving the speed\n",
        "model = torch.compile(model)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "\n",
        "\n",
        "time_sents = []\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  # another method for speed up, lower the precision for some of the operation\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = t1-t0\n",
        "  time_sents.append(dt)\n",
        "  print(f\"step: {i}, loss: {loss.item()}, time: {dt:.4f}\")\n",
        "\n",
        "# get the average time\n",
        "avg_time = np.mean(time_sents[1:50])\n",
        "print(f\"Average time: {avg_time:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkkgE2LYj4l-",
        "outputId": "103871c7-2148-402d-9553-55c2ba9e8fed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Number of batches: 18\n",
            "Number of tokens: 301966\n",
            "step: 0, loss: 10.934246063232422, time: 24.3486\n",
            "step: 1, loss: 9.439720153808594, time: 0.0944\n",
            "step: 2, loss: 9.042047500610352, time: 0.0943\n",
            "step: 3, loss: 8.909623146057129, time: 0.0943\n",
            "step: 4, loss: 8.652402877807617, time: 0.0945\n",
            "step: 5, loss: 8.543598175048828, time: 0.0943\n",
            "step: 6, loss: 8.34595775604248, time: 0.0944\n",
            "step: 7, loss: 8.137788772583008, time: 0.0945\n",
            "step: 8, loss: 7.843873500823975, time: 0.0946\n",
            "step: 9, loss: 7.622587203979492, time: 0.0941\n",
            "step: 10, loss: 7.421063423156738, time: 0.0944\n",
            "step: 11, loss: 7.318807601928711, time: 0.0943\n",
            "step: 12, loss: 7.150790214538574, time: 0.0941\n",
            "step: 13, loss: 7.102771759033203, time: 0.0949\n",
            "step: 14, loss: 7.051050662994385, time: 0.0946\n",
            "step: 15, loss: 6.89571475982666, time: 0.0945\n",
            "step: 16, loss: 6.862709999084473, time: 0.0944\n",
            "step: 17, loss: 6.815102577209473, time: 0.0943\n",
            "step: 18, loss: 6.6193695068359375, time: 0.0943\n",
            "step: 19, loss: 6.409982204437256, time: 0.0953\n",
            "step: 20, loss: 6.453614711761475, time: 0.0945\n",
            "step: 21, loss: 6.376561164855957, time: 0.0946\n",
            "step: 22, loss: 6.311905860900879, time: 0.0948\n",
            "step: 23, loss: 6.516427993774414, time: 0.0940\n",
            "step: 24, loss: 6.567907333374023, time: 0.0942\n",
            "step: 25, loss: 6.464414596557617, time: 0.0949\n",
            "step: 26, loss: 6.3899827003479, time: 0.0949\n",
            "step: 27, loss: 6.267598628997803, time: 0.0947\n",
            "step: 28, loss: 6.278900146484375, time: 0.0947\n",
            "step: 29, loss: 6.317905426025391, time: 0.0949\n",
            "step: 30, loss: 6.268612861633301, time: 0.0950\n",
            "step: 31, loss: 6.367404937744141, time: 0.0950\n",
            "step: 32, loss: 6.471640110015869, time: 0.0947\n",
            "step: 33, loss: 6.336516380310059, time: 0.0942\n",
            "step: 34, loss: 6.331720352172852, time: 0.0947\n",
            "step: 35, loss: 6.346318244934082, time: 0.0941\n",
            "step: 36, loss: 6.333522319793701, time: 0.0948\n",
            "step: 37, loss: 6.080598831176758, time: 0.0961\n",
            "step: 38, loss: 6.202176094055176, time: 0.0947\n",
            "step: 39, loss: 6.103671073913574, time: 0.0949\n",
            "step: 40, loss: 6.062226295471191, time: 0.0949\n",
            "step: 41, loss: 6.288764953613281, time: 0.0945\n",
            "step: 42, loss: 6.389692306518555, time: 0.0947\n",
            "step: 43, loss: 6.256930351257324, time: 0.0948\n",
            "step: 44, loss: 6.213310241699219, time: 0.0946\n",
            "step: 45, loss: 6.113218784332275, time: 0.0948\n",
            "step: 46, loss: 6.111016273498535, time: 0.0946\n",
            "step: 47, loss: 6.132198333740234, time: 0.0946\n",
            "step: 48, loss: 6.0653557777404785, time: 0.0946\n",
            "step: 49, loss: 6.235400676727295, time: 0.1185\n",
            "Average time: 0.0951\n"
          ]
        }
      ]
    }
  ]
}
